{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c1fce19a11f95416168ced03c2c70fa818b21a5",
    "colab_type": "text",
    "id": "KBeAf8WgaeSk"
   },
   "source": [
    "**Mask-RCNN Sample Starter Model for the RSNA Pneumonia Detection Challenge**\n",
    "\n",
    "[MD.ai](https://www.md.ai). The dataset for this challenge, created on the MD.ai platform in collaboration with the Radiological Society of North America (RSNA), the Society of Thoracic Radiology (STR), the US National Institutes of Health (NIH), and Kaggle.\n",
    "This notebook covers the basics of parsing the competition dataset, training using a detector basd on the [Mask-RCNN algorithm](https://arxiv.org/abs/1703.06870) for object detection and instance segmentation.    \n",
    "**Note that the Mask-RCNN detector configuration parameters have been selected to reduce training time for demonstration purposes, they are not optimal.\n",
    "**\n",
    "\n",
    "This is based on our deep learning for medical imaging lessons: \n",
    "\n",
    "- Lesson 1. Classification of chest vs. adominal X-rays using TensorFlow/Keras [Github](https://github.com/mdai/ml-lessons/blob/master/lesson1-xray-images-classification.ipynb) [Annotator](https://public.md.ai/annotator/project/PVq9raBJ)\n",
    "- Lesson 2. Lung X-Rays Semantic Segmentation using UNets. [Github](https://github.com/mdai/ml-lessons/blob/master/lesson2-lung-xrays-segmentation.ipynb)\n",
    "[Annotator](https://public.md.ai/annotator/project/aGq4k6NW/workspace) \n",
    "- Lesson 3. RSNA Pneumonia detection using Kaggle data format [Github](https://github.com/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-kaggle.ipynb) [Annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace) \n",
    "- Lesson 3. RSNA Pneumonia detection using MD.ai python client library [Github](https://github.com/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-mdai-client-lib.ipynb) [Annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace) \n",
    "\n",
    "*Copyright 2018 MD.ai, Inc.   \n",
    "Licensed under the Apache License, Version 2.0*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "40c67b3ff0fa04587dec508363308adaa3ceaf34",
    "colab": {},
    "colab_type": "code",
    "id": "4kjcC6QqywWl"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pydicom\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_cores():\n",
    "  nslots = os.getenv('NSLOTS')\n",
    "  if nslots is not None:\n",
    "    return int(nslots)\n",
    "  raise ValueError('Environment variable NSLOTS is not defined.')\n",
    "  \n",
    "session_conf = tf.ConfigProto(\n",
    "      intra_op_parallelism_threads=get_n_cores()-1,\n",
    "      inter_op_parallelism_threads=1,\n",
    "      allow_soft_placement=True, \n",
    "      log_device_placement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94926f47a24cab23f8fe1423884cfb1c607d150a",
    "colab_type": "text",
    "id": "eCXdW8QfjB81"
   },
   "source": [
    "### First: Install Kaggle API for download competition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "6e5764759e6a0a9b698b44645658f66873edd807",
    "colab": {},
    "colab_type": "code",
    "id": "yP0XLJx_x_6o"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/project/ece601/A2_Pneumonia_Detection/Dataset/'\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "ROOT_DIR = '/project/ece601/A2_Pneumonia_Detection_MaskRCNN/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5da6240cd31e4a99a6ad24d0d150aad333f83b5b",
    "colab_type": "text",
    "id": "KLTUyeTMTe4u"
   },
   "source": [
    "###  MD.ai Annotator \n",
    "\n",
    "Additionally, If you are interested in augmenting the existing annotations, you can use the MD.ai annotator to view DICOM images, and create annotatios to be exported.  \n",
    "MD.ai annotator project URL for the Kaggle dataset: https://public.md.ai/annotator/project/LxR6zdR2/workspace\n",
    "\n",
    "**Annotator features**\n",
    "- The annotator can be used to view DICOM images and create image and exam level annotations.\n",
    "- You can apply the annotator to filter by label, adjudicate annotations, and assign annotation tasks to your team.\n",
    "- Notebooks can be built directly within the annotator for rapid model development.\n",
    "- The data wrangling is abstracted away by the interface and by our MD.ai library.\n",
    "- Simplifies image annotation in order to widen the participation in the futrue of medical image deep learning.\n",
    "\n",
    "The annotator allows you to create initial annotations, build and run models, modify/finetune the annotations based on predicted values, and repeat.  \n",
    "The MD.ai python client library implements functions to easily download images and annotations and to prepare the datasets used to train the model for classification. See the following example notebook for parsing annotations and training using MD.ai annotator: \n",
    "https://github.com/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-mdai-client-lib.ipynb  \n",
    "- MD.ai URL: https://www.md.ai  \n",
    "- MD.ai documentation URL: https://docs.md.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "576df4c47a23d08b1bdb384245e09aa69f88bbd3",
    "colab_type": "text",
    "id": "kdYzLq1zfKL4"
   },
   "source": [
    "### Install Matterport's Mask-RCNN model from github.\n",
    "See the [Matterport's implementation of Mask-RCNN](https://github.com/matterport/Mask_RCNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "3acbbbe055b6a409d3c50ae0f893acf51b5ae7ba",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-KZXyWwhzOVU",
    "outputId": "2576cc17-7484-4311-ad72-3c5643dcb5bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Mask RCNN\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "50089cc61791871cdf6a5c0037dc4f28b7b7d7cc",
    "colab": {},
    "colab_type": "code",
    "id": "FghMmiMjzOX2"
   },
   "outputs": [],
   "source": [
    "train_dicom_dir = os.path.join(DATA_DIR, 'stage_1_train_images')\n",
    "test_dicom_dir = os.path.join(DATA_DIR, 'stage_1_test_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "032cc5fe4baa051108106675e6ca4f4fdb2846ed",
    "colab_type": "text",
    "id": "gj-tvDvEaDiC"
   },
   "source": [
    "### Some setup functions and classes for Mask-RCNN\n",
    "\n",
    "- dicom_fps is a list of the dicom image path and filenames \n",
    "- image_annotions is a dictionary of the annotations keyed by the filenames\n",
    "- parsing the dataset returns a list of the image filenames and the annotations dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "778cb19865d7cc63440491aef9202b71c61e8bb2",
    "colab": {},
    "colab_type": "code",
    "id": "ivqC4cnszOaM"
   },
   "outputs": [],
   "source": [
    "def get_dicom_fps(dicom_dir):\n",
    "    dicom_fps = glob.glob(dicom_dir+'/'+'*.dcm')\n",
    "    return list(set(dicom_fps))\n",
    "\n",
    "def parse_dataset(dicom_dir, anns): \n",
    "    image_fps = get_dicom_fps(dicom_dir)\n",
    "    image_annotations = {fp: [] for fp in image_fps}\n",
    "    for index, row in anns.iterrows(): \n",
    "        fp = os.path.join(dicom_dir, row['patientId']+'.dcm')\n",
    "        image_annotations[fp].append(row)\n",
    "    return image_fps, image_annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "dfcffc4eaa94a41497717851dee9f702d8a2a73b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "colab_type": "code",
    "id": "_SfzTa-1zOck",
    "outputId": "91ae8935-bccb-4b8e-9a7e-aa690f95fd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        3\n",
      "DETECTION_MIN_CONFIDENCE       0.9\n",
      "DETECTION_NMS_THRESHOLD        0.1\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  256\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [256 256   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               3\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           pneumonia\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following parameters have been selected to reduce running time for demonstration purposes \n",
    "# These are not optimal \n",
    "\n",
    "class DetectorConfig(Config):\n",
    "    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n",
    "    Overrides values in the base Config class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Give the configuration a recognizable name  \n",
    "    NAME = 'pneumonia'\n",
    "    \n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8 \n",
    "    \n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    NUM_CLASSES = 2  # background + 1 pneumonia classes\n",
    "    \n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256)\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    MAX_GT_INSTANCES = 3\n",
    "    DETECTION_MAX_INSTANCES = 3\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    DETECTION_NMS_THRESHOLD = 0.1\n",
    "\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    \n",
    "config = DetectorConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "52bd3ffbdde0173a363055482d675da51c2aba99",
    "colab": {},
    "colab_type": "code",
    "id": "8EBVA1M60yAj"
   },
   "outputs": [],
   "source": [
    "class DetectorDataset(utils.Dataset):\n",
    "    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Add classes\n",
    "        self.add_class('pneumonia', 1, 'Lung Opacity')\n",
    "   \n",
    "        # add images \n",
    "        for i, fp in enumerate(image_fps):\n",
    "            annotations = image_annotations[fp]\n",
    "            self.add_image('pneumonia', image_id=i, path=fp, \n",
    "                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n",
    "            \n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        fp = info['path']\n",
    "        ds = pydicom.read_file(fp)\n",
    "        image = ds.pixel_array\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "            image = np.stack((image,) * 3, -1)\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        annotations = info['annotations']\n",
    "        count = len(annotations)\n",
    "        if count == 0:\n",
    "            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n",
    "            class_ids = np.zeros((1,), dtype=np.int32)\n",
    "        else:\n",
    "            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n",
    "            class_ids = np.zeros((count,), dtype=np.int32)\n",
    "            for i, a in enumerate(annotations):\n",
    "                if a['Target'] == 1:\n",
    "                    x = int(a['x'])\n",
    "                    y = int(a['y'])\n",
    "                    w = int(a['width'])\n",
    "                    h = int(a['height'])\n",
    "                    mask_instance = mask[:, :, i].copy()\n",
    "                    cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)\n",
    "                    mask[:, :, i] = mask_instance\n",
    "                    class_ids[i] = 1\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cb852e262b69d348743767d675573368ab672c9",
    "colab_type": "text",
    "id": "9RlMo04ckd98"
   },
   "source": [
    "### Examine the annotation data, parse the dataset, and view dicom fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "793b1c6c6ba4e5f0d51e130080aa799f230b5ef6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "EdhUEFDr0yDA",
    "outputId": "1715a5df-a577-41fd-bf20-f1a27aadb28c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003d8fa0-6bf1-40ed-b54c-ac657f8495c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00436515-870c-4b36-a041-de91049b9ab4</td>\n",
       "      <td>264.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              patientId      x      y  width  height  Target\n",
       "0  0004cfab-14fd-4e49-80ba-63a80b6bddd6    NaN    NaN    NaN     NaN       0\n",
       "1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd    NaN    NaN    NaN     NaN       0\n",
       "2  00322d4d-1c29-4943-afc9-b6754be640eb    NaN    NaN    NaN     NaN       0\n",
       "3  003d8fa0-6bf1-40ed-b54c-ac657f8495c5    NaN    NaN    NaN     NaN       0\n",
       "4  00436515-870c-4b36-a041-de91049b9ab4  264.0  152.0  213.0   379.0       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training dataset\n",
    "anns = pd.read_csv(os.path.join(DATA_DIR, 'stage_1_train_labels.csv'))\n",
    "anns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "7aebc88f910b232e3b8759421914a007c6ffed94",
    "colab": {},
    "colab_type": "code",
    "id": "Mxz-pNbt5txY"
   },
   "outputs": [],
   "source": [
    "image_fps, image_annotations = parse_dataset(train_dicom_dir, anns=anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6c386dcef041b972f6209dd19e247d547c3c349f",
    "colab": {},
    "colab_type": "code",
    "id": "YPqjEIXWRhSf"
   },
   "outputs": [],
   "source": [
    "ds = pydicom.read_file(image_fps[0]) # read dicom image from filepath \n",
    "image = ds.pixel_array # get image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "0ef68a41cf1a5e842e86a219b6392e3695004720",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "colab_type": "code",
    "id": "81lovwF2Ro5R",
    "outputId": "e2263fe2-1a32-432a-ec75-b9220a24e697"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0008, 0005) Specific Character Set              CS: 'ISO_IR 100'\n",
       "(0008, 0016) SOP Class UID                       UI: Secondary Capture Image Storage\n",
       "(0008, 0018) SOP Instance UID                    UI: 1.2.276.0.7230010.3.1.4.8323329.29506.1517874492.259348\n",
       "(0008, 0020) Study Date                          DA: '19010101'\n",
       "(0008, 0030) Study Time                          TM: '000000.00'\n",
       "(0008, 0050) Accession Number                    SH: ''\n",
       "(0008, 0060) Modality                            CS: 'CR'\n",
       "(0008, 0064) Conversion Type                     CS: 'WSD'\n",
       "(0008, 0090) Referring Physician's Name          PN: ''\n",
       "(0008, 103e) Series Description                  LO: 'view: PA'\n",
       "(0010, 0010) Patient's Name                      PN: '8949f382-c28f-462b-a902-058818235e3b'\n",
       "(0010, 0020) Patient ID                          LO: '8949f382-c28f-462b-a902-058818235e3b'\n",
       "(0010, 0030) Patient's Birth Date                DA: ''\n",
       "(0010, 0040) Patient's Sex                       CS: 'F'\n",
       "(0010, 1010) Patient's Age                       AS: '48'\n",
       "(0018, 0015) Body Part Examined                  CS: 'CHEST'\n",
       "(0018, 5101) View Position                       CS: 'PA'\n",
       "(0020, 000d) Study Instance UID                  UI: 1.2.276.0.7230010.3.1.2.8323329.29506.1517874492.259347\n",
       "(0020, 000e) Series Instance UID                 UI: 1.2.276.0.7230010.3.1.3.8323329.29506.1517874492.259346\n",
       "(0020, 0010) Study ID                            SH: ''\n",
       "(0020, 0011) Series Number                       IS: \"1\"\n",
       "(0020, 0013) Instance Number                     IS: \"1\"\n",
       "(0020, 0020) Patient Orientation                 CS: ''\n",
       "(0028, 0002) Samples per Pixel                   US: 1\n",
       "(0028, 0004) Photometric Interpretation          CS: 'MONOCHROME2'\n",
       "(0028, 0010) Rows                                US: 1024\n",
       "(0028, 0011) Columns                             US: 1024\n",
       "(0028, 0030) Pixel Spacing                       DS: ['0.14300000000000002', '0.14300000000000002']\n",
       "(0028, 0100) Bits Allocated                      US: 8\n",
       "(0028, 0101) Bits Stored                         US: 8\n",
       "(0028, 0102) High Bit                            US: 7\n",
       "(0028, 0103) Pixel Representation                US: 0\n",
       "(0028, 2110) Lossy Image Compression             CS: '01'\n",
       "(0028, 2114) Lossy Image Compression Method      CS: 'ISO_10918_1'\n",
       "(7fe0, 0010) Pixel Data                          OB: Array of 158948 bytes"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dicom fields \n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "74277ae9af4a3b044e62b664d10d76b23848bb43",
    "colab": {},
    "colab_type": "code",
    "id": "gYNSd1AhRqOV"
   },
   "outputs": [],
   "source": [
    "# Original DICOM image size: 1024 x 1024\n",
    "ORIG_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6563bbca143e4bceb1ea850714d7b43bb1e1178d",
    "colab_type": "text",
    "id": "4FlRu8ML-ceg"
   },
   "source": [
    "### Split the data into training and validation datasets\n",
    "**Note: We have only used only a portion of the images for demonstration purposes. See comments below.**\n",
    " \n",
    " - To use all the images do: image_fps_list = list(image_fps)\n",
    " - Or change the number of images from 100 to a custom number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "6175c72e73639e3190e127f67783988eadced9ba",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7jByVCZt-ZOC",
    "outputId": "f1aa267d-7530-4620-ffc5-2f7aa39083bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23115 2569\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Modify this line to use more or fewer images for training/validation. \n",
    "# To use all images, do: image_fps_list = list(image_fps)\n",
    "image_fps_list = list(image_fps) \n",
    "#####################################################################\n",
    "\n",
    "# split dataset into training vs. validation dataset \n",
    "# split ratio is set to 0.9 vs. 0.1 (train vs. validation, respectively)\n",
    "sorted(image_fps_list)\n",
    "random.seed(42)\n",
    "random.shuffle(image_fps_list)\n",
    "\n",
    "validation_split = (1/10)\n",
    "split_index = int((1 - validation_split) * len(image_fps_list))\n",
    "\n",
    "image_fps_train = image_fps_list[:split_index]\n",
    "image_fps_val = image_fps_list[split_index:]\n",
    "\n",
    "print(len(image_fps_train), len(image_fps_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5143c19dc22bc00d318a3b28cb7e13c7fbacc8a",
    "colab_type": "text",
    "id": "9KUvacUbgiEX"
   },
   "source": [
    "### Create and prepare the training dataset using the DetectorDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "86c3333d4dfb8b7d00ce1f401693d0df4e6254e1",
    "colab": {},
    "colab_type": "code",
    "id": "jwMkhotP0yFf"
   },
   "outputs": [],
   "source": [
    "# prepare the training dataset\n",
    "dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\n",
    "dataset_train.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f69286e6b0b640827a3de166326d157a6d86668",
    "colab_type": "text",
    "id": "wPDQ9EVDgxa6"
   },
   "source": [
    "### Let's look at a sample annotation. We see a bounding box with (x, y) of the the top left corner as well as the width and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "93da5a58731ad483a4bd2b20543f2b1df4b8ad74",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "0xEc47Jz59x5",
    "outputId": "129edfbc-cf9d-46c7-b569-d804a50cd12d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[patientId    7670693d-7f1d-4045-8d61-77a68bb58e91\n",
       " x                                             NaN\n",
       " y                                             NaN\n",
       " width                                         NaN\n",
       " height                                        NaN\n",
       " Target                                          0\n",
       " Name: 11389, dtype: object]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show annotation(s) for a DICOM image \n",
    "test_fp = random.choice(image_fps_train)\n",
    "image_annotations[test_fp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "313347d838fa8321a714858c8073f98c50c5be26",
    "colab": {},
    "colab_type": "code",
    "id": "K1TkWuGP0yHl"
   },
   "outputs": [],
   "source": [
    "# prepare the validation dataset\n",
    "dataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "600a8135d4e382f62797d69e9358f5697873c8f9",
    "colab_type": "text",
    "id": "pEXEt8fygWuC"
   },
   "source": [
    "### Display a random image with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "491b78ec96d28fcdbbf8e2d7f9320a05d64c9249",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "4xwsrf9G1lHR",
    "outputId": "a13386d3-a918-41fe-8824-13625c9d7b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024, 3)\n",
      "/project/ece601/A2_Pneumonia_Detection/Dataset/stage_1_train_images/32f2b5b8-fb22-4fb1-bfcc-692dd530f917.dcm\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Load and display random samples and their bounding boxes\n",
    "# Suggestion: Run this a few times to see different examples. \n",
    "\n",
    "image_id = random.choice(dataset_train.image_ids)\n",
    "image_fp = dataset_train.image_reference(image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image[:, :, 0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "masked = np.zeros(image.shape[:2])\n",
    "for i in range(mask.shape[2]):\n",
    "    masked += image[:, :, 0] * mask[:, :, i]\n",
    "plt.imshow(masked, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "print(image_fp)\n",
    "print(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "ac5e11c89daeaa4e73af6a4967a4a375cddf284c",
    "colab": {},
    "colab_type": "code",
    "id": "geTvh0sU1lJo"
   },
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "342b6008873fe7a6a0870a712ee47a87f0d2828d",
    "colab_type": "text",
    "id": "ustAIH78hZI_"
   },
   "source": [
    "### Image Augmentation. Try finetuning some variables to custom values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ab9d6086ce611a46f189c047956c43b29783e6d",
    "colab": {},
    "colab_type": "code",
    "id": "STZnQTE61lME"
   },
   "outputs": [],
   "source": [
    "# Image augmentation \n",
    "augmentation = iaa.SomeOf((0, 1), [\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-25, 25),\n",
    "        shear=(-8, 8)\n",
    "    ),\n",
    "    iaa.Multiply((0.9, 1.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e65d2cecb283f446f34cdde19b663a8a8e9590f",
    "colab_type": "text",
    "id": "M4kt7LKuc78e"
   },
   "source": [
    "### Now it's time to train the model. Note that training even a basic model can take a few hours. \n",
    "\n",
    "Note: the following model is for demonstration purpose only. We have limited the training to one epoch, and have set nominal values for the Detector Configuration to reduce run-time. \n",
    "\n",
    "- dataset_train and dataset_val are derived from DetectorDataset \n",
    "- DetectorDataset loads images from image filenames and  masks from the annotation data\n",
    "- model is Mask-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64cce2581ffdb8c2b1cb07948ada4a93f64874b0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2575
    },
    "colab_type": "code",
    "id": "RVgNhHjl1lOS",
    "outputId": "2cba9efc-eeea-472d-d155-3c3d856585bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /project/ece601/A2_Pneumonia_Detection_MaskRCNN/pneumonia20181204T1237/mask_rcnn_pneumonia_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 114s 1s/step - loss: 2.6612 - rpn_class_loss: 0.2909 - rpn_bbox_loss: 0.7554 - mrcnn_class_loss: 0.3451 - mrcnn_bbox_loss: 0.6686 - mrcnn_mask_loss: 0.6013 - val_loss: 2.4039 - val_rpn_class_loss: 0.0757 - val_rpn_bbox_loss: 0.7217 - val_mrcnn_class_loss: 0.3675 - val_mrcnn_bbox_loss: 0.6810 - val_mrcnn_mask_loss: 0.5580\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 1.6243 - rpn_class_loss: 0.0551 - rpn_bbox_loss: 0.2709 - mrcnn_class_loss: 0.2938 - mrcnn_bbox_loss: 0.4515 - mrcnn_mask_loss: 0.5529 - val_loss: 2.2636 - val_rpn_class_loss: 0.0609 - val_rpn_bbox_loss: 0.6752 - val_mrcnn_class_loss: 0.3341 - val_mrcnn_bbox_loss: 0.6636 - val_mrcnn_mask_loss: 0.5298\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 51s 510ms/step - loss: 1.6046 - rpn_class_loss: 0.0549 - rpn_bbox_loss: 0.2640 - mrcnn_class_loss: 0.2911 - mrcnn_bbox_loss: 0.4398 - mrcnn_mask_loss: 0.5548 - val_loss: 2.5736 - val_rpn_class_loss: 0.0510 - val_rpn_bbox_loss: 0.9345 - val_mrcnn_class_loss: 0.3814 - val_mrcnn_bbox_loss: 0.6850 - val_mrcnn_mask_loss: 0.5218\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 51s 508ms/step - loss: 1.5040 - rpn_class_loss: 0.0390 - rpn_bbox_loss: 0.2471 - mrcnn_class_loss: 0.2599 - mrcnn_bbox_loss: 0.4078 - mrcnn_mask_loss: 0.5503 - val_loss: 2.5390 - val_rpn_class_loss: 0.0613 - val_rpn_bbox_loss: 0.9808 - val_mrcnn_class_loss: 0.2581 - val_mrcnn_bbox_loss: 0.6764 - val_mrcnn_mask_loss: 0.5624\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 1.3533 - rpn_class_loss: 0.0409 - rpn_bbox_loss: 0.1764 - mrcnn_class_loss: 0.2518 - mrcnn_bbox_loss: 0.3851 - mrcnn_mask_loss: 0.4990 - val_loss: 2.3030 - val_rpn_class_loss: 0.0483 - val_rpn_bbox_loss: 0.9753 - val_mrcnn_class_loss: 0.2828 - val_mrcnn_bbox_loss: 0.5135 - val_mrcnn_mask_loss: 0.4831\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 52s 517ms/step - loss: 1.5625 - rpn_class_loss: 0.0416 - rpn_bbox_loss: 0.2831 - mrcnn_class_loss: 0.2923 - mrcnn_bbox_loss: 0.4311 - mrcnn_mask_loss: 0.5144 - val_loss: 2.3632 - val_rpn_class_loss: 0.0441 - val_rpn_bbox_loss: 0.9565 - val_mrcnn_class_loss: 0.2817 - val_mrcnn_bbox_loss: 0.5871 - val_mrcnn_mask_loss: 0.4938\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 52s 516ms/step - loss: 1.5888 - rpn_class_loss: 0.0439 - rpn_bbox_loss: 0.3051 - mrcnn_class_loss: 0.2811 - mrcnn_bbox_loss: 0.4366 - mrcnn_mask_loss: 0.5221 - val_loss: 1.9780 - val_rpn_class_loss: 0.0565 - val_rpn_bbox_loss: 0.5501 - val_mrcnn_class_loss: 0.3375 - val_mrcnn_bbox_loss: 0.5532 - val_mrcnn_mask_loss: 0.4806\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 1.3810 - rpn_class_loss: 0.0348 - rpn_bbox_loss: 0.2369 - mrcnn_class_loss: 0.2398 - mrcnn_bbox_loss: 0.3874 - mrcnn_mask_loss: 0.4821 - val_loss: 2.7174 - val_rpn_class_loss: 0.0566 - val_rpn_bbox_loss: 1.1834 - val_mrcnn_class_loss: 0.3550 - val_mrcnn_bbox_loss: 0.6366 - val_mrcnn_mask_loss: 0.4857\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 51s 510ms/step - loss: 1.3389 - rpn_class_loss: 0.0316 - rpn_bbox_loss: 0.2262 - mrcnn_class_loss: 0.2395 - mrcnn_bbox_loss: 0.3739 - mrcnn_mask_loss: 0.4677 - val_loss: 1.9918 - val_rpn_class_loss: 0.0362 - val_rpn_bbox_loss: 0.5115 - val_mrcnn_class_loss: 0.4578 - val_mrcnn_bbox_loss: 0.5101 - val_mrcnn_mask_loss: 0.4761\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 51s 508ms/step - loss: 1.4542 - rpn_class_loss: 0.0411 - rpn_bbox_loss: 0.2527 - mrcnn_class_loss: 0.2691 - mrcnn_bbox_loss: 0.4138 - mrcnn_mask_loss: 0.4776 - val_loss: 2.4740 - val_rpn_class_loss: 0.0632 - val_rpn_bbox_loss: 0.7960 - val_mrcnn_class_loss: 0.4579 - val_mrcnn_bbox_loss: 0.7008 - val_mrcnn_mask_loss: 0.4562\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 1.3771 - rpn_class_loss: 0.0334 - rpn_bbox_loss: 0.2552 - mrcnn_class_loss: 0.2379 - mrcnn_bbox_loss: 0.3909 - mrcnn_mask_loss: 0.4598 - val_loss: 3.1531 - val_rpn_class_loss: 0.0662 - val_rpn_bbox_loss: 1.2583 - val_mrcnn_class_loss: 0.4517 - val_mrcnn_bbox_loss: 0.8396 - val_mrcnn_mask_loss: 0.5372\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 52s 516ms/step - loss: 1.4368 - rpn_class_loss: 0.0337 - rpn_bbox_loss: 0.2729 - mrcnn_class_loss: 0.2513 - mrcnn_bbox_loss: 0.3931 - mrcnn_mask_loss: 0.4858 - val_loss: 2.9453 - val_rpn_class_loss: 0.0688 - val_rpn_bbox_loss: 1.5324 - val_mrcnn_class_loss: 0.3133 - val_mrcnn_bbox_loss: 0.5477 - val_mrcnn_mask_loss: 0.4832\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 52s 515ms/step - loss: 1.4648 - rpn_class_loss: 0.0383 - rpn_bbox_loss: 0.2923 - mrcnn_class_loss: 0.2666 - mrcnn_bbox_loss: 0.4037 - mrcnn_mask_loss: 0.4638 - val_loss: 2.2928 - val_rpn_class_loss: 0.0533 - val_rpn_bbox_loss: 0.9421 - val_mrcnn_class_loss: 0.2555 - val_mrcnn_bbox_loss: 0.5765 - val_mrcnn_mask_loss: 0.4654\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 51s 508ms/step - loss: 1.3360 - rpn_class_loss: 0.0328 - rpn_bbox_loss: 0.2599 - mrcnn_class_loss: 0.2410 - mrcnn_bbox_loss: 0.3495 - mrcnn_mask_loss: 0.4528 - val_loss: 2.3514 - val_rpn_class_loss: 0.0502 - val_rpn_bbox_loss: 0.9751 - val_mrcnn_class_loss: 0.2890 - val_mrcnn_bbox_loss: 0.5860 - val_mrcnn_mask_loss: 0.4510\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 51s 507ms/step - loss: 1.5067 - rpn_class_loss: 0.0342 - rpn_bbox_loss: 0.3184 - mrcnn_class_loss: 0.2686 - mrcnn_bbox_loss: 0.4253 - mrcnn_mask_loss: 0.4602 - val_loss: 2.1883 - val_rpn_class_loss: 0.0437 - val_rpn_bbox_loss: 0.7665 - val_mrcnn_class_loss: 0.3055 - val_mrcnn_bbox_loss: 0.5935 - val_mrcnn_mask_loss: 0.4791\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 52s 518ms/step - loss: 1.3976 - rpn_class_loss: 0.0353 - rpn_bbox_loss: 0.2260 - mrcnn_class_loss: 0.2746 - mrcnn_bbox_loss: 0.3890 - mrcnn_mask_loss: 0.4726 - val_loss: 2.0747 - val_rpn_class_loss: 0.0450 - val_rpn_bbox_loss: 0.6275 - val_mrcnn_class_loss: 0.3168 - val_mrcnn_bbox_loss: 0.5936 - val_mrcnn_mask_loss: 0.4918\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 52s 524ms/step - loss: 1.3965 - rpn_class_loss: 0.0331 - rpn_bbox_loss: 0.2460 - mrcnn_class_loss: 0.2860 - mrcnn_bbox_loss: 0.3784 - mrcnn_mask_loss: 0.4529 - val_loss: 2.3506 - val_rpn_class_loss: 0.0443 - val_rpn_bbox_loss: 0.8714 - val_mrcnn_class_loss: 0.3403 - val_mrcnn_bbox_loss: 0.6224 - val_mrcnn_mask_loss: 0.4722\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 51s 508ms/step - loss: 1.2787 - rpn_class_loss: 0.0230 - rpn_bbox_loss: 0.2385 - mrcnn_class_loss: 0.2234 - mrcnn_bbox_loss: 0.3503 - mrcnn_mask_loss: 0.4435 - val_loss: 2.1104 - val_rpn_class_loss: 0.0507 - val_rpn_bbox_loss: 0.7731 - val_mrcnn_class_loss: 0.2896 - val_mrcnn_bbox_loss: 0.5667 - val_mrcnn_mask_loss: 0.4302\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 51s 506ms/step - loss: 1.1798 - rpn_class_loss: 0.0290 - rpn_bbox_loss: 0.2288 - mrcnn_class_loss: 0.2062 - mrcnn_bbox_loss: 0.3018 - mrcnn_mask_loss: 0.4139 - val_loss: 2.5964 - val_rpn_class_loss: 0.0640 - val_rpn_bbox_loss: 0.8944 - val_mrcnn_class_loss: 0.4849 - val_mrcnn_bbox_loss: 0.6774 - val_mrcnn_mask_loss: 0.4756\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 51s 514ms/step - loss: 1.1944 - rpn_class_loss: 0.0271 - rpn_bbox_loss: 0.1852 - mrcnn_class_loss: 0.2338 - mrcnn_bbox_loss: 0.3278 - mrcnn_mask_loss: 0.4205 - val_loss: 2.1299 - val_rpn_class_loss: 0.0322 - val_rpn_bbox_loss: 0.6705 - val_mrcnn_class_loss: 0.3458 - val_mrcnn_bbox_loss: 0.6203 - val_mrcnn_mask_loss: 0.4611\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 49s 494ms/step - loss: 1.2142 - rpn_class_loss: 0.0234 - rpn_bbox_loss: 0.2002 - mrcnn_class_loss: 0.2292 - mrcnn_bbox_loss: 0.3304 - mrcnn_mask_loss: 0.4311 - val_loss: 1.8798 - val_rpn_class_loss: 0.0313 - val_rpn_bbox_loss: 0.5665 - val_mrcnn_class_loss: 0.2570 - val_mrcnn_bbox_loss: 0.5279 - val_mrcnn_mask_loss: 0.4971\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 51s 508ms/step - loss: 1.1988 - rpn_class_loss: 0.0264 - rpn_bbox_loss: 0.1760 - mrcnn_class_loss: 0.2342 - mrcnn_bbox_loss: 0.3235 - mrcnn_mask_loss: 0.4387 - val_loss: 2.6048 - val_rpn_class_loss: 0.0428 - val_rpn_bbox_loss: 1.2065 - val_mrcnn_class_loss: 0.2989 - val_mrcnn_bbox_loss: 0.5492 - val_mrcnn_mask_loss: 0.5073\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 1.2788 - rpn_class_loss: 0.0322 - rpn_bbox_loss: 0.2214 - mrcnn_class_loss: 0.2409 - mrcnn_bbox_loss: 0.3596 - mrcnn_mask_loss: 0.4248 - val_loss: 2.3720 - val_rpn_class_loss: 0.0276 - val_rpn_bbox_loss: 0.7719 - val_mrcnn_class_loss: 0.2842 - val_mrcnn_bbox_loss: 0.8201 - val_mrcnn_mask_loss: 0.4682\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 50s 495ms/step - loss: 1.2455 - rpn_class_loss: 0.0277 - rpn_bbox_loss: 0.1832 - mrcnn_class_loss: 0.2494 - mrcnn_bbox_loss: 0.3470 - mrcnn_mask_loss: 0.4382 - val_loss: 2.0780 - val_rpn_class_loss: 0.0347 - val_rpn_bbox_loss: 0.7607 - val_mrcnn_class_loss: 0.2739 - val_mrcnn_bbox_loss: 0.5265 - val_mrcnn_mask_loss: 0.4823\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 51s 507ms/step - loss: 1.3791 - rpn_class_loss: 0.0352 - rpn_bbox_loss: 0.2668 - mrcnn_class_loss: 0.2620 - mrcnn_bbox_loss: 0.3815 - mrcnn_mask_loss: 0.4336 - val_loss: 2.3631 - val_rpn_class_loss: 0.0372 - val_rpn_bbox_loss: 0.9427 - val_mrcnn_class_loss: 0.2789 - val_mrcnn_bbox_loss: 0.6131 - val_mrcnn_mask_loss: 0.4912\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 50s 502ms/step - loss: 1.2607 - rpn_class_loss: 0.0351 - rpn_bbox_loss: 0.2277 - mrcnn_class_loss: 0.2454 - mrcnn_bbox_loss: 0.3289 - mrcnn_mask_loss: 0.4237 - val_loss: 1.8059 - val_rpn_class_loss: 0.0295 - val_rpn_bbox_loss: 0.5363 - val_mrcnn_class_loss: 0.3120 - val_mrcnn_bbox_loss: 0.4997 - val_mrcnn_mask_loss: 0.4284\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 50s 503ms/step - loss: 1.2722 - rpn_class_loss: 0.0264 - rpn_bbox_loss: 0.2171 - mrcnn_class_loss: 0.2558 - mrcnn_bbox_loss: 0.3430 - mrcnn_mask_loss: 0.4299 - val_loss: 2.0650 - val_rpn_class_loss: 0.0426 - val_rpn_bbox_loss: 0.6709 - val_mrcnn_class_loss: 0.3339 - val_mrcnn_bbox_loss: 0.5555 - val_mrcnn_mask_loss: 0.4621\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 51s 507ms/step - loss: 1.2838 - rpn_class_loss: 0.0259 - rpn_bbox_loss: 0.2236 - mrcnn_class_loss: 0.2352 - mrcnn_bbox_loss: 0.3525 - mrcnn_mask_loss: 0.4466 - val_loss: 2.7566 - val_rpn_class_loss: 0.0494 - val_rpn_bbox_loss: 0.9844 - val_mrcnn_class_loss: 0.5024 - val_mrcnn_bbox_loss: 0.6948 - val_mrcnn_mask_loss: 0.5255\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 1.2284 - rpn_class_loss: 0.0220 - rpn_bbox_loss: 0.1945 - mrcnn_class_loss: 0.2562 - mrcnn_bbox_loss: 0.3211 - mrcnn_mask_loss: 0.4346 - val_loss: 2.4583 - val_rpn_class_loss: 0.0352 - val_rpn_bbox_loss: 0.9217 - val_mrcnn_class_loss: 0.3614 - val_mrcnn_bbox_loss: 0.6429 - val_mrcnn_mask_loss: 0.4971\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 51s 508ms/step - loss: 1.1175 - rpn_class_loss: 0.0222 - rpn_bbox_loss: 0.1853 - mrcnn_class_loss: 0.1942 - mrcnn_bbox_loss: 0.3006 - mrcnn_mask_loss: 0.4153 - val_loss: 1.8246 - val_rpn_class_loss: 0.0287 - val_rpn_bbox_loss: 0.6412 - val_mrcnn_class_loss: 0.2470 - val_mrcnn_bbox_loss: 0.4851 - val_mrcnn_mask_loss: 0.4227\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 50s 502ms/step - loss: 1.3040 - rpn_class_loss: 0.0170 - rpn_bbox_loss: 0.2741 - mrcnn_class_loss: 0.2441 - mrcnn_bbox_loss: 0.3449 - mrcnn_mask_loss: 0.4240 - val_loss: 2.1380 - val_rpn_class_loss: 0.0326 - val_rpn_bbox_loss: 0.9731 - val_mrcnn_class_loss: 0.1797 - val_mrcnn_bbox_loss: 0.4904 - val_mrcnn_mask_loss: 0.4621\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 49s 493ms/step - loss: 1.3547 - rpn_class_loss: 0.0289 - rpn_bbox_loss: 0.2659 - mrcnn_class_loss: 0.2683 - mrcnn_bbox_loss: 0.3554 - mrcnn_mask_loss: 0.4362 - val_loss: 2.0008 - val_rpn_class_loss: 0.0298 - val_rpn_bbox_loss: 0.7445 - val_mrcnn_class_loss: 0.2842 - val_mrcnn_bbox_loss: 0.4999 - val_mrcnn_mask_loss: 0.4423\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 1.1747 - rpn_class_loss: 0.0195 - rpn_bbox_loss: 0.1787 - mrcnn_class_loss: 0.2275 - mrcnn_bbox_loss: 0.3182 - mrcnn_mask_loss: 0.4307 - val_loss: 2.2219 - val_rpn_class_loss: 0.0476 - val_rpn_bbox_loss: 0.7706 - val_mrcnn_class_loss: 0.3708 - val_mrcnn_bbox_loss: 0.5185 - val_mrcnn_mask_loss: 0.5144\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 51s 509ms/step - loss: 1.2075 - rpn_class_loss: 0.0228 - rpn_bbox_loss: 0.1969 - mrcnn_class_loss: 0.2351 - mrcnn_bbox_loss: 0.3370 - mrcnn_mask_loss: 0.4156 - val_loss: 2.1603 - val_rpn_class_loss: 0.0303 - val_rpn_bbox_loss: 0.6396 - val_mrcnn_class_loss: 0.4609 - val_mrcnn_bbox_loss: 0.5838 - val_mrcnn_mask_loss: 0.4456\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 1.1304 - rpn_class_loss: 0.0231 - rpn_bbox_loss: 0.1796 - mrcnn_class_loss: 0.2173 - mrcnn_bbox_loss: 0.3027 - mrcnn_mask_loss: 0.4076 - val_loss: 2.3638 - val_rpn_class_loss: 0.0391 - val_rpn_bbox_loss: 0.8292 - val_mrcnn_class_loss: 0.4601 - val_mrcnn_bbox_loss: 0.5762 - val_mrcnn_mask_loss: 0.4592\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 1.1533 - rpn_class_loss: 0.0212 - rpn_bbox_loss: 0.2114 - mrcnn_class_loss: 0.2037 - mrcnn_bbox_loss: 0.3060 - mrcnn_mask_loss: 0.4110 - val_loss: 1.9929 - val_rpn_class_loss: 0.0223 - val_rpn_bbox_loss: 0.5018 - val_mrcnn_class_loss: 0.3596 - val_mrcnn_bbox_loss: 0.6359 - val_mrcnn_mask_loss: 0.4734\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 51s 512ms/step - loss: 1.2485 - rpn_class_loss: 0.0257 - rpn_bbox_loss: 0.2567 - mrcnn_class_loss: 0.2256 - mrcnn_bbox_loss: 0.3344 - mrcnn_mask_loss: 0.4060 - val_loss: 2.1431 - val_rpn_class_loss: 0.0347 - val_rpn_bbox_loss: 0.7055 - val_mrcnn_class_loss: 0.3675 - val_mrcnn_bbox_loss: 0.5903 - val_mrcnn_mask_loss: 0.4451\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 50s 501ms/step - loss: 1.3592 - rpn_class_loss: 0.0279 - rpn_bbox_loss: 0.2242 - mrcnn_class_loss: 0.3004 - mrcnn_bbox_loss: 0.3761 - mrcnn_mask_loss: 0.4306 - val_loss: 2.2276 - val_rpn_class_loss: 0.0366 - val_rpn_bbox_loss: 1.0318 - val_mrcnn_class_loss: 0.2036 - val_mrcnn_bbox_loss: 0.4890 - val_mrcnn_mask_loss: 0.4665\n",
      "Epoch 39/100\n",
      " 77/100 [======================>.......] - ETA: 9s - loss: 1.2392 - rpn_class_loss: 0.0238 - rpn_bbox_loss: 0.2727 - mrcnn_class_loss: 0.2403 - mrcnn_bbox_loss: 0.3026 - mrcnn_mask_loss: 0.3999"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Train Mask-RCNN Model \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=NUM_EPOCHS, \n",
    "            layers='all',\n",
    "            augmentation=augmentation)\n",
    "history = model.keras_model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1,len(next(iter(history.values())))+1)\n",
    "pd.DataFrame(history, index=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(epochs, history[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\n",
    "plt.legend()\n",
    "plt.subplot(132)\n",
    "plt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train class ce\")\n",
    "plt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid class ce\")\n",
    "plt.legend()\n",
    "plt.subplot(133)\n",
    "plt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train box loss\")\n",
    "plt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid box loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db5c10d3f7da099e5751a04a6e6d49819882ecd4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eraRlzgPmmIZ",
    "outputId": "de9e688c-ba4f-4b62-f842-dbcf00ce397c"
   },
   "outputs": [],
   "source": [
    "# select trained model \n",
    "dir_names = next(os.walk(model.model_dir))[1]\n",
    "key = config.NAME.lower()\n",
    "dir_names = filter(lambda f: f.startswith(key), dir_names)\n",
    "dir_names = sorted(dir_names)\n",
    "\n",
    "if not dir_names:\n",
    "    import errno\n",
    "    raise FileNotFoundError(\n",
    "        errno.ENOENT,\n",
    "        \"Could not find model directory under {}\".format(self.model_dir))\n",
    "    \n",
    "fps = []\n",
    "# Pick last directory\n",
    "for d in dir_names: \n",
    "    dir_name = os.path.join(model.model_dir, d)\n",
    "    # Find the last checkpoint\n",
    "    checkpoints = next(os.walk(dir_name))[2]\n",
    "    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n",
    "    checkpoints = sorted(checkpoints)\n",
    "    if not checkpoints:\n",
    "        print('No weight files in {}'.format(dir_name))\n",
    "    else: \n",
    "      \n",
    "      checkpoint = os.path.join(dir_name, checkpoints[-1])\n",
    "      fps.append(checkpoint)\n",
    "\n",
    "model_path = sorted(fps)[-1]\n",
    "print('Found model {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52138636b2ae5bf444bba808518cd8313bde65cd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "TgpT9AzC2Bgz",
    "outputId": "60f5a175-4666-497d-b4e8-0bdab39a92d0"
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(DetectorConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode='inference', \n",
    "                          config=inference_config,\n",
    "                          model_dir=ROOT_DIR)\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e13c61bee23b791c61ecf1256f7512295cd4d9ab",
    "colab": {},
    "colab_type": "code",
    "id": "9mTBig7D2BjU"
   },
   "outputs": [],
   "source": [
    "# set color for class\n",
    "def get_colors_for_class_ids(class_ids):\n",
    "    colors = []\n",
    "    for class_id in class_ids:\n",
    "        if class_id == 1:\n",
    "            colors.append((.941, .204, .204))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f99fbd3f31ff1a2bd66764835c9b646375364598",
    "colab_type": "text",
    "id": "A8EiL2LOiCr_"
   },
   "source": [
    "### How does the predicted box compared to the expected value? Let's use the validation dataset to check. \n",
    "\n",
    "Note that we trained only one epoch for **demonstration purposes ONLY**. You might be able to improve performance running more epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "186412199e25b98719f71cfe5e8869abcce516c4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1394
    },
    "colab_type": "code",
    "id": "irheTbrW2Bl0",
    "outputId": "56041ad4-173d-45ab-af67-f54e8333511e"
   },
   "outputs": [],
   "source": [
    "# Show few example of ground truth vs. predictions on the validation dataset \n",
    "dataset = dataset_val\n",
    "fig = plt.figure(figsize=(10, 30))\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    image_id = random.choice(dataset.image_ids)\n",
    "    \n",
    "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config, \n",
    "                               image_id, use_mini_mask=False)\n",
    "    \n",
    "    print(original_image.shape)\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                                dataset.class_names,\n",
    "                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n",
    "    \n",
    "    plt.subplot(6, 2, 2*i + 2)\n",
    "    results = model.detect([original_image]) #, verbose=1)\n",
    "    r = results[0]\n",
    "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset.class_names, r['scores'], \n",
    "                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd9f53fa319a425693e07fe4898ddeeaa5d07f99",
    "colab": {},
    "colab_type": "code",
    "id": "qRWBVJKYNdWM"
   },
   "outputs": [],
   "source": [
    "# Get filenames of test dataset DICOM images\n",
    "test_image_fps = get_dicom_fps(test_dicom_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "164e18701a830bc6c42a791feea13549de37289b",
    "colab_type": "text",
    "id": "WcV1cL_aiSc4"
   },
   "source": [
    "### Final steps - Create the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a5c0c6134408ddbf5a34496d7e9d7be5692e9a1",
    "colab": {},
    "colab_type": "code",
    "id": "C6UWVrbM2Bob"
   },
   "outputs": [],
   "source": [
    "# Make predictions on test images, write out sample submission \n",
    "def predict(image_fps, filepath='submission.csv', min_conf=0.95): \n",
    "    \n",
    "    # assume square image\n",
    "    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n",
    "    #resize_factor = ORIG_SIZE \n",
    "    with open(filepath, 'w') as file:\n",
    "      for image_id in tqdm(image_fps): \n",
    "        ds = pydicom.read_file(image_id)\n",
    "        image = ds.pixel_array\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "            image = np.stack((image,) * 3, -1) \n",
    "        image, window, scale, padding, crop = utils.resize_image(\n",
    "            image,\n",
    "            min_dim=config.IMAGE_MIN_DIM,\n",
    "            min_scale=config.IMAGE_MIN_SCALE,\n",
    "            max_dim=config.IMAGE_MAX_DIM,\n",
    "            mode=config.IMAGE_RESIZE_MODE)\n",
    "            \n",
    "        patient_id = os.path.splitext(os.path.basename(image_id))[0]\n",
    "\n",
    "        results = model.detect([image])\n",
    "        r = results[0]\n",
    "\n",
    "        out_str = \"\"\n",
    "        out_str += patient_id \n",
    "        out_str += \",\"\n",
    "        assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n",
    "        if len(r['rois']) == 0: \n",
    "            pass\n",
    "        else: \n",
    "            num_instances = len(r['rois'])\n",
    "  \n",
    "            for i in range(num_instances): \n",
    "                if r['scores'][i] > min_conf: \n",
    "                    out_str += ' '\n",
    "                    out_str += str(round(r['scores'][i], 2))\n",
    "                    out_str += ' '\n",
    "\n",
    "                    # x1, y1, width, height \n",
    "                    x1 = r['rois'][i][1]\n",
    "                    y1 = r['rois'][i][0]\n",
    "                    width = r['rois'][i][3] - x1 \n",
    "                    height = r['rois'][i][2] - y1 \n",
    "                    bboxes_str = \"{} {} {} {}\".format(x1*resize_factor, y1*resize_factor, \\\n",
    "                                                       width*resize_factor, height*resize_factor)   \n",
    "#                     bboxes_str = \"{} {} {} {}\".format(x1, y1, \\\n",
    "#                                                       width, height)\n",
    "                    out_str += bboxes_str\n",
    "\n",
    "        file.write(out_str+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0406e7f5aaa4867782c4f9c064f90bba386128e7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "C5cBpNka2Bsv",
    "outputId": "a2af9176-d9d6-49f6-f22a-5a1c455d144f"
   },
   "outputs": [],
   "source": [
    "# predict only the first 50 entries\n",
    "submission_fp = os.path.join(ROOT_DIR, 'submission.csv')\n",
    "print(submission_fp)\n",
    "predict(test_image_fps, filepath=submission_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fd8d178fc51ef0bca94fbb3f423160f08a77edc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1599
    },
    "colab_type": "code",
    "id": "_BjPE_Ee9rbA",
    "outputId": "67b5f053-112b-494a-9ab3-d017bfb440c2"
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv(submission_fp, names=['patientId', 'PredictionString'])\n",
    "output.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e015ce023a544d2aa38b9b98940b302d4c93def"
   },
   "outputs": [],
   "source": [
    "## show submission.csv content\n",
    "os.chdir(ROOT_DIR)\n",
    "!cat submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea110f197abc2acb1c3435383f7259079dc0eb0e"
   },
   "outputs": [],
   "source": [
    "# show a few test image detection example\n",
    "def visualize(): \n",
    "    image_id = random.choice(test_image_fps)\n",
    "    ds = pydicom.read_file(image_id)\n",
    "    \n",
    "    # original image \n",
    "    image = ds.pixel_array\n",
    "    \n",
    "    # assume square image \n",
    "    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n",
    "    \n",
    "    # If grayscale. Convert to RGB for consistency.\n",
    "    if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "        image = np.stack((image,) * 3, -1) \n",
    "    resized_image, window, scale, padding, crop = utils.resize_image(\n",
    "        image,\n",
    "        min_dim=config.IMAGE_MIN_DIM,\n",
    "        min_scale=config.IMAGE_MIN_SCALE,\n",
    "        max_dim=config.IMAGE_MAX_DIM,\n",
    "        mode=config.IMAGE_RESIZE_MODE)\n",
    "\n",
    "    patient_id = os.path.splitext(os.path.basename(image_id))[0]\n",
    "    print(patient_id)\n",
    "\n",
    "    results = model.detect([resized_image])\n",
    "    r = results[0]\n",
    "    for bbox in r['rois']: \n",
    "        print(bbox)\n",
    "        x1 = int(bbox[1] * resize_factor)\n",
    "        y1 = int(bbox[0] * resize_factor)\n",
    "        x2 = int(bbox[3] * resize_factor)\n",
    "        y2 = int(bbox[2]  * resize_factor)\n",
    "        cv2.rectangle(image, (x1,y1), (x2,y2), (77, 255, 9), 3, 1)\n",
    "        width = x2 - x1 \n",
    "        height = y2 - y1 \n",
    "        print(\"x {} y {} h {} w {}\".format(x1, y1, width, height))\n",
    "    plt.figure() \n",
    "    plt.imshow(image, cmap=plt.cm.gist_gray)\n",
    "\n",
    "visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "835a15c9d018acd5deb16e9e02f9b765f68d0e78"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lesson-3-rsna-pneumonia-detection-challenge-kaggle",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
